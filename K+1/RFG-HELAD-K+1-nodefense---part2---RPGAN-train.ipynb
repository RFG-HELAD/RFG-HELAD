{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07597ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 21:15:04) \n",
      "[GCC 7.3.0]\n",
      "1.8.0+cu111\n",
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6f7cf72090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os, random, time, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import misc\n",
    "from scipy import ndimage, signal\n",
    "import scipy\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a5a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./savedata/RPGAN\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device ='cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda:0'\n",
    "\n",
    "\n",
    "batch_size = 16    \n",
    "bestEpoch = 4  \n",
    "lr = 0.0001 \n",
    "num_epochs = 5\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "save_dir=\"./savedata/RPGAN\"\n",
    "\n",
    "print(save_dir)    \n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "log_filename = os.path.join(save_dir, 'train.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8523e686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data.............................\n",
      "torch.Size([10309, 3, 32, 32]) torch.Size([2578, 3, 32, 32]) torch.Size([10309]) torch.Size([2578])\n",
      "All down Train Data: 10309\n",
      "Real train Data: 8923\n",
      "All testsetA Data: 2578\n",
      "Real testsetA Data: 2226\n",
      "All testsetA Data: 2226\n",
      "Real testsetD Data: 352\n",
      "All testsetC Data: 2578\n",
      "Real testsetC Data: 2578\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('==> Preparing data.............................')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import MNIST, CIFAR10, CIFAR100, SVHN\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "import  numpy as np\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Safeman(Dataset):\n",
    "    \n",
    "    def __init__(self, data,targets):\n",
    "        super(Safeman, self).__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        return img, target\n",
    "\n",
    "class Safeman_Filter(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(known.index(targets[i]))\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "        \n",
    "class Safeman_FilterB(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        new_targets = []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                new_targets.append(0)\n",
    "            else:\n",
    "                new_targets.append(1)\n",
    "        self.targets = np.array(new_targets)\n",
    "        self.data = self.data\n",
    "\n",
    "class Safeman_FilterC(Safeman):\n",
    "    \n",
    "    def __Filter__(self, trainknown):\n",
    "        train_class_num=len(trainknown)\n",
    "        for i in range(0,len(self.targets)) :\n",
    "            if self.targets[i]>train_class_num:\n",
    "                self.targets[i] = train_class_num\n",
    "        self.data = self.data\n",
    "\n",
    "        \n",
    "class Safeman_FilterD(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        train_class_num=len(known)\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] not in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(train_class_num)\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "\n",
    "\n",
    "        \n",
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(8)\n",
    "\n",
    "\n",
    "\n",
    "known=[0, 1, 2,3,4]\n",
    "unknown=[ 5,6,7]\n",
    "\n",
    "num_class=len(known)\n",
    "\n",
    "\n",
    "\n",
    "X_train0 = np.load('./dataset/X_train_ukm.npy')\n",
    "y_train1 = np.load('./dataset/y_train_ukm.npy')\n",
    "X_final_test0 = np.load('./dataset/X_final_test_ukm.npy' )\n",
    "y__final_test1 = np.load('./dataset/y__final_test_ukm.npy')\n",
    "\n",
    "X_train1=[]\n",
    "X_final_test1=[]\n",
    "\n",
    "for i in range(len(y_train1)):\n",
    "    a = np.resize(X_train0[i], (3, 32, 32))\n",
    "    X_train1 += [a]\n",
    "    \n",
    "for j in range(len(y__final_test1)):\n",
    "    b = np.resize(X_final_test0[j], (3, 32, 32))\n",
    "    X_final_test1 += [b]\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train,y_test = torch.Tensor(X_train1), torch.Tensor(X_final_test1), torch.from_numpy(y_train1), torch.from_numpy(y__final_test1)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape,y_test.shape)\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_dataset.data = train_dataset.tensors[0]\n",
    "train_dataset.targets = train_dataset.tensors[1]\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "test_dataset.data = test_dataset.tensors[0]\n",
    "test_dataset.targets = test_dataset.tensors[1]\n",
    "\n",
    "\n",
    "\n",
    "labels =['ARP poisining', 'BeEF HTTP exploits','Mass HTTP requests','Metasploit exploits','Normal','Port scanning','TCP flood','UDP data flood']\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.classes = labels\n",
    "test_dataset.classes = labels\n",
    "\n",
    "train_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "test_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "\n",
    "b_s=16\n",
    "\n",
    "trainset = Safeman_Filter(data=train_dataset.data,targets=train_dataset.targets)\n",
    "print('All down Train Data:', len(trainset))\n",
    "trainset.__Filter__(known=known)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=b_s, shuffle=True,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real train Data:', len(trainset))\n",
    "\n",
    "\n",
    "\n",
    "testsetA = Safeman_Filter(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetA.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_A = torch.utils.data.DataLoader(\n",
    "    testsetA, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetA Data:', len(testsetA))\n",
    "\n",
    "\n",
    "testsetD = Safeman_FilterD(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetD.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_D = torch.utils.data.DataLoader(\n",
    "    testsetD, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetD Data:', len(testsetD))\n",
    "\n",
    "\n",
    "testsetC = Safeman_FilterC(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetC Data:', len(testsetC))\n",
    "testsetC.__Filter__(trainknown=known)\n",
    "\n",
    "\n",
    "test_loader_C = torch.utils.data.DataLoader(\n",
    "    testsetC, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetC Data:', len(testsetC))\n",
    "\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d388fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "class Generatorzy(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generatorzy, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(z_dim, 256*8*8)\n",
    "        self.g_deconv_1 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(256, 128, kernel_size=3,\n",
    "                                    stride= 2, padding=(3-2+1)//2,\n",
    "                                    output_padding = (3-2)%2), \n",
    "                          nn.BatchNorm2d(128),\n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.g_deconv_2 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(128, 64, kernel_size=3,\n",
    "                                    stride= 1, padding=(3-1+1)//2,\n",
    "                                    output_padding = (3-1)%2), \n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.g_deconv_3 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(64, 3, kernel_size=3,\n",
    "                                    stride= 2, padding=(3-2+1)//2,\n",
    "                                    output_padding = (3-2)%2),\n",
    "                          nn.Tanh()\n",
    "                          )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x).view(-1, 256, 8, 8)\n",
    "        x = self.g_deconv_1(x)\n",
    "        x = self.g_deconv_2(x)\n",
    "        x = self.g_deconv_3(x)\n",
    "        \n",
    "        return x     \n",
    "\n",
    "\n",
    "    \n",
    "class Discriminatorzy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminatorzy, self).__init__()\n",
    "        \n",
    "        self.d_conv_1 = nn.Sequential(\n",
    "                          nn.Conv2d(3, 32, kernel_size=3,\n",
    "                                    stride=2, padding=1), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.d_conv_2 = nn.Sequential(\n",
    "                          nn.Conv2d(32, 64, kernel_size=3,\n",
    "                                    stride=2, padding=1), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.d_conv_3 = nn.Sequential(\n",
    "                          nn.Conv2d(64, 128, kernel_size=3,\n",
    "                                    stride=2, padding=0), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.fc = nn.Linear(3*3*128, 1)\n",
    "        self.fczy = nn.Linear(3*3*128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.d_conv_1(x)\n",
    "        x = self.d_conv_2(x)\n",
    "        x = self.d_conv_3(x)\n",
    "        x = x.view(-1, 128*3*3)\n",
    "        x_zy = self.fczy(x)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x_zy,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f6f559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)     \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "z_dim=100\n",
    "batch_size=b_s\n",
    "discriminator = Discriminatorzy()\n",
    "netDzy = discriminator.to(device)\n",
    "\n",
    "discriminatoraux = Discriminatorzy()\n",
    "netDzyaux = discriminatoraux.to(device)\n",
    "\n",
    "generator = Generatorzy(z_dim)\n",
    "netGzy = generator.to(device)\n",
    "\n",
    "\n",
    "netDzy.apply(weights_init)\n",
    "\n",
    "netDzyaux.apply(weights_init)\n",
    "\n",
    "netGzy.apply(weights_init)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "beta1 = 0.5\n",
    "\n",
    "optimizerD = optim.Adam(netDzy.parameters(), lr=lr/1.5, betas=(beta1, 0.999))\n",
    "\n",
    "optimizerDaux = optim.Adam(netDzyaux.parameters(), lr=lr/1.5, betas=(beta1, 0.999))\n",
    "\n",
    "optimizerG = optim.Adam(netGzy.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b4eb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/557]\terrD_real: 0.7071\terrDaux_real: 0.6862\terrD_fake: 0.6790\terrDaux_fake: 0.7001 \terrGqiuhe: 0.6389\n",
      "[0/5][50/557]\terrD_real: 0.6845\terrDaux_real: 0.5846\terrD_fake: 0.6809\terrDaux_fake: 0.6322 \terrGqiuhe: 0.6301\n",
      "[0/5][100/557]\terrD_real: 0.6714\terrDaux_real: 0.4682\terrD_fake: 0.6520\terrDaux_fake: 0.5065 \terrGqiuhe: 0.6525\n",
      "[0/5][150/557]\terrD_real: 0.6639\terrDaux_real: 0.3359\terrD_fake: 0.6350\terrDaux_fake: 0.3370 \terrGqiuhe: 0.6355\n",
      "[0/5][200/557]\terrD_real: 0.6545\terrDaux_real: 0.2616\terrD_fake: 0.6329\terrDaux_fake: 0.3631 \terrGqiuhe: 0.6460\n",
      "[0/5][250/557]\terrD_real: 0.6117\terrDaux_real: 0.1535\terrD_fake: 0.5758\terrDaux_fake: 0.2052 \terrGqiuhe: 0.7006\n",
      "[0/5][300/557]\terrD_real: 0.6177\terrDaux_real: 0.0814\terrD_fake: 0.5636\terrDaux_fake: 0.1072 \terrGqiuhe: 0.6143\n",
      "[0/5][350/557]\terrD_real: 0.5395\terrDaux_real: 0.0773\terrD_fake: 0.5790\terrDaux_fake: 0.0903 \terrGqiuhe: 0.6044\n",
      "[0/5][400/557]\terrD_real: 0.5859\terrDaux_real: 0.1228\terrD_fake: 0.6955\terrDaux_fake: 0.8444 \terrGqiuhe: 0.5264\n",
      "[0/5][450/557]\terrD_real: 0.6682\terrDaux_real: 0.0668\terrD_fake: 0.6214\terrDaux_fake: 0.0730 \terrGqiuhe: 0.4954\n",
      "[0/5][500/557]\terrD_real: 0.5659\terrDaux_real: 0.0434\terrD_fake: 0.5310\terrDaux_fake: 0.0424 \terrGqiuhe: 0.5600\n",
      "[0/5][550/557]\terrD_real: 0.6312\terrDaux_real: 0.0499\terrD_fake: 0.6017\terrDaux_fake: 0.0560 \terrGqiuhe: 0.4967\n",
      "[1/5][0/557]\terrD_real: 0.6364\terrDaux_real: 0.0658\terrD_fake: 0.6111\terrDaux_fake: 0.0562 \terrGqiuhe: 0.4699\n",
      "[1/5][50/557]\terrD_real: 0.5861\terrDaux_real: 0.0628\terrD_fake: 0.5595\terrDaux_fake: 0.0383 \terrGqiuhe: 0.5558\n",
      "[1/5][100/557]\terrD_real: 0.5083\terrDaux_real: 0.0391\terrD_fake: 0.5947\terrDaux_fake: 0.0622 \terrGqiuhe: 0.6064\n",
      "[1/5][150/557]\terrD_real: 0.6634\terrDaux_real: 0.0712\terrD_fake: 0.6090\terrDaux_fake: 0.0814 \terrGqiuhe: 0.4906\n",
      "[1/5][200/557]\terrD_real: 0.6765\terrDaux_real: 0.0486\terrD_fake: 0.6375\terrDaux_fake: 0.0443 \terrGqiuhe: 0.3883\n",
      "[1/5][250/557]\terrD_real: 0.6941\terrDaux_real: 0.0670\terrD_fake: 0.6141\terrDaux_fake: 0.0559 \terrGqiuhe: 0.4105\n",
      "[1/5][300/557]\terrD_real: 0.6941\terrDaux_real: 0.0436\terrD_fake: 0.5715\terrDaux_fake: 0.0754 \terrGqiuhe: 0.4521\n",
      "[1/5][350/557]\terrD_real: 0.6974\terrDaux_real: 0.0368\terrD_fake: 0.5509\terrDaux_fake: 0.0335 \terrGqiuhe: 0.3829\n",
      "[1/5][400/557]\terrD_real: 0.6559\terrDaux_real: 0.0287\terrD_fake: 0.5303\terrDaux_fake: 0.0353 \terrGqiuhe: 0.3846\n",
      "[1/5][450/557]\terrD_real: 0.4915\terrDaux_real: 0.0314\terrD_fake: 0.5770\terrDaux_fake: 0.0093 \terrGqiuhe: 0.4880\n",
      "[1/5][500/557]\terrD_real: 0.7618\terrDaux_real: 0.0405\terrD_fake: 0.5074\terrDaux_fake: 0.0655 \terrGqiuhe: 0.2506\n",
      "[1/5][550/557]\terrD_real: 0.5219\terrDaux_real: 0.0477\terrD_fake: 0.6000\terrDaux_fake: 0.0200 \terrGqiuhe: 0.4655\n",
      "[2/5][0/557]\terrD_real: 0.7584\terrDaux_real: 0.0458\terrD_fake: 0.4821\terrDaux_fake: 0.0265 \terrGqiuhe: 0.1914\n",
      "[2/5][50/557]\terrD_real: 0.5160\terrDaux_real: 0.0189\terrD_fake: 0.5495\terrDaux_fake: 0.0195 \terrGqiuhe: 0.5479\n",
      "[2/5][100/557]\terrD_real: 0.4563\terrDaux_real: 0.0134\terrD_fake: 0.5690\terrDaux_fake: 0.0226 \terrGqiuhe: 0.6284\n",
      "[2/5][150/557]\terrD_real: 0.4575\terrDaux_real: 0.0172\terrD_fake: 0.5205\terrDaux_fake: 0.0218 \terrGqiuhe: 0.6489\n",
      "[2/5][200/557]\terrD_real: 0.3653\terrDaux_real: 0.0067\terrD_fake: 0.6020\terrDaux_fake: 0.0166 \terrGqiuhe: 0.7456\n",
      "[2/5][250/557]\terrD_real: 0.3953\terrDaux_real: 0.0155\terrD_fake: 0.5235\terrDaux_fake: 0.0375 \terrGqiuhe: 0.6931\n",
      "[2/5][300/557]\terrD_real: 0.5294\terrDaux_real: 0.0126\terrD_fake: 0.4996\terrDaux_fake: 0.0716 \terrGqiuhe: 0.3408\n",
      "[2/5][350/557]\terrD_real: 0.4517\terrDaux_real: 0.0358\terrD_fake: 0.4385\terrDaux_fake: 0.0115 \terrGqiuhe: 0.6213\n",
      "[2/5][400/557]\terrD_real: 0.3762\terrDaux_real: 0.0238\terrD_fake: 0.5408\terrDaux_fake: 0.0136 \terrGqiuhe: 0.7927\n",
      "[2/5][450/557]\terrD_real: 0.4355\terrDaux_real: 0.0099\terrD_fake: 0.4348\terrDaux_fake: 0.0320 \terrGqiuhe: 0.4969\n",
      "[2/5][500/557]\terrD_real: 0.4473\terrDaux_real: 0.0095\terrD_fake: 0.3367\terrDaux_fake: 0.0227 \terrGqiuhe: 0.6491\n",
      "[2/5][550/557]\terrD_real: 0.4740\terrDaux_real: 0.0440\terrD_fake: 0.3958\terrDaux_fake: 0.0914 \terrGqiuhe: 0.5318\n",
      "[3/5][0/557]\terrD_real: 0.7074\terrDaux_real: 0.6142\terrD_fake: 0.6838\terrDaux_fake: 1.0455 \terrGqiuhe: 0.6219\n",
      "[3/5][50/557]\terrD_real: 0.7811\terrDaux_real: 0.8223\terrD_fake: 0.6457\terrDaux_fake: 0.9473 \terrGqiuhe: 0.6129\n",
      "[3/5][100/557]\terrD_real: 0.3915\terrDaux_real: 0.7084\terrD_fake: 0.6584\terrDaux_fake: 0.7292 \terrGqiuhe: 1.1171\n",
      "[3/5][150/557]\terrD_real: 0.4731\terrDaux_real: 0.7021\terrD_fake: 0.4699\terrDaux_fake: 0.6982 \terrGqiuhe: 1.0144\n",
      "[3/5][200/557]\terrD_real: 0.3786\terrDaux_real: 0.6996\terrD_fake: 0.6796\terrDaux_fake: 0.6966 \terrGqiuhe: 1.2110\n",
      "[3/5][250/557]\terrD_real: 0.1617\terrDaux_real: 0.6974\terrD_fake: 0.8643\terrDaux_fake: 0.6933 \terrGqiuhe: 1.6664\n",
      "[3/5][300/557]\terrD_real: 0.8111\terrDaux_real: 0.6979\terrD_fake: 0.2506\terrDaux_fake: 0.6943 \terrGqiuhe: 0.7028\n",
      "[3/5][350/557]\terrD_real: 0.3254\terrDaux_real: 0.6966\terrD_fake: 0.5329\terrDaux_fake: 0.6938 \terrGqiuhe: 1.3136\n",
      "[3/5][400/557]\terrD_real: 0.6228\terrDaux_real: 0.6976\terrD_fake: 0.2526\terrDaux_fake: 0.6978 \terrGqiuhe: 1.0684\n",
      "[3/5][450/557]\terrD_real: 0.6776\terrDaux_real: 0.6966\terrD_fake: 0.3502\terrDaux_fake: 0.6992 \terrGqiuhe: 0.8397\n",
      "[3/5][500/557]\terrD_real: 0.4462\terrDaux_real: 0.6960\terrD_fake: 0.4005\terrDaux_fake: 0.6960 \terrGqiuhe: 1.1657\n",
      "[3/5][550/557]\terrD_real: 0.3394\terrDaux_real: 0.6962\terrD_fake: 0.5445\terrDaux_fake: 0.6939 \terrGqiuhe: 1.1091\n",
      "[4/5][0/557]\terrD_real: 0.2875\terrDaux_real: 0.6953\terrD_fake: 0.4782\terrDaux_fake: 0.6930 \terrGqiuhe: 1.4638\n",
      "[4/5][50/557]\terrD_real: 0.3337\terrDaux_real: 0.6948\terrD_fake: 0.4622\terrDaux_fake: 0.6955 \terrGqiuhe: 1.3430\n",
      "[4/5][100/557]\terrD_real: 0.4886\terrDaux_real: 0.6958\terrD_fake: 0.3425\terrDaux_fake: 0.6938 \terrGqiuhe: 1.0554\n",
      "[4/5][150/557]\terrD_real: 0.2320\terrDaux_real: 0.6944\terrD_fake: 0.4088\terrDaux_fake: 0.6943 \terrGqiuhe: 1.3346\n",
      "[4/5][200/557]\terrD_real: 0.4544\terrDaux_real: 0.6951\terrD_fake: 0.3851\terrDaux_fake: 0.6966 \terrGqiuhe: 1.0183\n",
      "[4/5][250/557]\terrD_real: 0.5664\terrDaux_real: 0.6941\terrD_fake: 0.2551\terrDaux_fake: 0.6971 \terrGqiuhe: 0.9791\n",
      "[4/5][300/557]\terrD_real: 0.5719\terrDaux_real: 0.6956\terrD_fake: 0.2039\terrDaux_fake: 0.6944 \terrGqiuhe: 0.9629\n",
      "[4/5][350/557]\terrD_real: 0.6350\terrDaux_real: 0.6945\terrD_fake: 0.3052\terrDaux_fake: 0.6913 \terrGqiuhe: 1.0205\n",
      "[4/5][400/557]\terrD_real: 0.4164\terrDaux_real: 0.6944\terrD_fake: 0.3648\terrDaux_fake: 0.6898 \terrGqiuhe: 1.3363\n",
      "[4/5][450/557]\terrD_real: 0.8694\terrDaux_real: 0.6952\terrD_fake: 0.2153\terrDaux_fake: 0.6916 \terrGqiuhe: 0.7354\n",
      "[4/5][500/557]\terrD_real: 0.2380\terrDaux_real: 0.6959\terrD_fake: 0.2648\terrDaux_fake: 0.6952 \terrGqiuhe: 1.4430\n",
      "[4/5][550/557]\terrD_real: 0.3658\terrDaux_real: 0.6943\terrD_fake: 0.3522\terrDaux_fake: 0.6986 \terrGqiuhe: 1.3732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "dataloader_train_closeset=train_loader\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    i=0\n",
    "    for sample in dataloader_train_closeset:\n",
    "        \n",
    "        data, datalabel = sample\n",
    "        ############################\n",
    "        # (1) Update D1 D2 network\n",
    "        ###########################\n",
    "        \n",
    "        netDzy.zero_grad()\n",
    "        netDzyaux.zero_grad()\n",
    "        \n",
    "        real_cpu = data.to(device)\n",
    "        bb_size = real_cpu.size(0)\n",
    "        \n",
    "        label = torch.full((bb_size,), real_label, device=device)\n",
    "        labelaux = torch.full((bb_size,), fake_label, device=device)\n",
    "        \n",
    "        _,output = netDzy(real_cpu)\n",
    "        output=output.view(-1)\n",
    "        \n",
    "        _,outputaux = netDzyaux(real_cpu)\n",
    "        outputaux=outputaux.view(-1)\n",
    "        \n",
    "        output=output.to(torch.float32)\n",
    "        outputaux=outputaux.to(torch.float32) \n",
    "    \n",
    "        label=label.to(torch.float32)\n",
    "        labelaux=labelaux.to(torch.float32)\n",
    "        \n",
    "        errD_real = criterion(output, label)        \n",
    "        errDaux_real = criterion(outputaux, labelaux)\n",
    "\n",
    "        errD_real.backward()        \n",
    "        errDaux_real.backward()\n",
    "        \n",
    "\n",
    "        noise = torch.randn(batch_size, z_dim, device=device)\n",
    "\n",
    "        fake = netGzy(noise)\n",
    "        label.fill_(fake_label)\n",
    "        \n",
    "        labelaux.fill_(real_label)\n",
    "\n",
    "        _,output = netDzy(fake.detach())\n",
    "        \n",
    "        output=output.view(-1)\n",
    "\n",
    "        output=output.to(torch.float32)\n",
    "        \n",
    "        _,outputaux = netDzyaux(fake.detach())\n",
    "        outputaux=outputaux.view(-1)\n",
    "        \n",
    "        outputaux=outputaux.to(torch.float32)\n",
    "        \n",
    "        label=label.to(torch.float32)\n",
    "        labelaux=labelaux.to(torch.float32)\n",
    "        \n",
    "        errD_fake = criterion(output, label)\n",
    "        \n",
    "        errDaux_fake = criterion(outputaux, labelaux)\n",
    "        \n",
    "        errD_fake.backward()\n",
    "        \n",
    "        errDaux_fake.backward()\n",
    "        \n",
    "        \n",
    "        optimizerD.step()\n",
    "        optimizerDaux.step()\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        netGzy.zero_grad()\n",
    "        label.fill_(real_label)  \n",
    "        \n",
    "        labelaux.fill_(fake_label) \n",
    "        \n",
    "\n",
    "        _,output = netDzy(fake)\n",
    "        output=output.view(-1)\n",
    "\n",
    "        output=output.to(torch.float32)\n",
    "        label=label.to(torch.float32)\n",
    "        errG = criterion(output, label)\n",
    "        \n",
    "\n",
    "        _,outputaux = netDzyaux(fake)\n",
    "        outputaux=outputaux.view(-1)\n",
    "\n",
    "        outputaux=outputaux.to(torch.float32)\n",
    "        labelaux=labelaux.to(torch.float32)\n",
    "        errGaux = criterion(outputaux, labelaux)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        errGqiuhe=errG-errGaux*0.1  \n",
    "        errGqiuhe.backward()\n",
    "\n",
    "\n",
    "        optimizerG.step()\n",
    "          \n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\terrD_real: %.4f\\terrDaux_real: %.4f\\terrD_fake: %.4f\\terrDaux_fake: %.4f \\terrGqiuhe: %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader_train_closeset),\n",
    "                     errD_real, errDaux_real,errD_fake,errDaux_fake, errGqiuhe))\n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "    cur_model_wts = copy.deepcopy(netGzy.state_dict())\n",
    "    path_to_save_paramOnly = os.path.join(save_dir, 'RPGAN-epoch-{}.GNet'.format(epoch+1))\n",
    "    torch.save(cur_model_wts, path_to_save_paramOnly)\n",
    "    \n",
    "    cur_model_wts = copy.deepcopy(netDzy.state_dict())\n",
    "    path_to_save_paramOnly = os.path.join(save_dir, 'RPGAN-epoch-{}.DNet'.format(epoch+1))\n",
    "    torch.save(cur_model_wts, path_to_save_paramOnly)\n",
    "    \n",
    "   \n",
    "    cur_model_wts = copy.deepcopy(netDzyaux.state_dict())\n",
    "    path_to_save_paramOnly = os.path.join(save_dir, 'RPGAN-epoch-{}.DauxNet'.format(epoch+1))\n",
    "    torch.save(cur_model_wts, path_to_save_paramOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cf52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
