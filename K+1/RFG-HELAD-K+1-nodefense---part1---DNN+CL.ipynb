{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0md9re01ABZA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from SupConLosszy import SupConLoss\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data.............................\n",
      "torch.Size([10309, 1, 28, 28]) torch.Size([2578, 1, 28, 28]) torch.Size([10309]) torch.Size([2578])\n",
      "All down Train Data: 10309\n",
      "Real train Data: 8923\n",
      "All testsetA Data: 2578\n",
      "Real testsetA Data: 2226\n",
      "All testsetA Data: 2226\n",
      "Real testsetD Data: 352\n",
      "All testsetC Data: 2578\n",
      "Real testsetC Data: 2578\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data.............................')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import MNIST, CIFAR10, CIFAR100, SVHN\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "import  numpy as np\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Safeman(Dataset):\n",
    "    \n",
    "    def __init__(self, data,targets):\n",
    "        super(Safeman, self).__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        return img, target\n",
    "\n",
    "class Safeman_Filter(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(known.index(targets[i]))\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "        \n",
    "class Safeman_FilterB(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        new_targets = []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                new_targets.append(0)\n",
    "            else:\n",
    "                new_targets.append(1)\n",
    "        self.targets = np.array(new_targets)\n",
    "        self.data = self.data\n",
    "\n",
    "class Safeman_FilterC(Safeman):\n",
    "    \n",
    "    def __Filter__(self, trainknown):\n",
    "        train_class_num=len(trainknown)\n",
    "        for i in range(0,len(self.targets)) :\n",
    "            if self.targets[i]>train_class_num:\n",
    "                self.targets[i] = train_class_num\n",
    "        self.data = self.data\n",
    "\n",
    "\n",
    "class Safeman_FilterD(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        train_class_num=len(known)\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] not in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(train_class_num)\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)        \n",
    "        \n",
    "\n",
    "        \n",
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(999)\n",
    "\n",
    "\n",
    "\n",
    "known=[0, 1, 2,3,4]\n",
    "unknown=[ 5,6,7]\n",
    "\n",
    "num_class=len(known)\n",
    "\n",
    "\n",
    "\n",
    "X_train0 = np.load('./dataset/X_train_ukm.npy')\n",
    "y_train1 = np.load('./dataset/y_train_ukm.npy')\n",
    "X_final_test0 = np.load('./dataset/X_final_test_ukm.npy' )\n",
    "y__final_test1 = np.load('./dataset/y__final_test_ukm.npy')\n",
    "\n",
    "X_train1=[]\n",
    "X_final_test1=[]\n",
    "\n",
    "for i in range(len(y_train1)):\n",
    "    a = np.resize(X_train0[i], (1, 28, 28))\n",
    "    X_train1 += [a]\n",
    "    \n",
    "for j in range(len(y__final_test1)):\n",
    "    b = np.resize(X_final_test0[j], (1, 28, 28))\n",
    "    X_final_test1 += [b]\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train,y_test = torch.Tensor(X_train1), torch.Tensor(X_final_test1), torch.from_numpy(y_train1), torch.from_numpy(y__final_test1)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape,y_test.shape)\n",
    "\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_dataset.data = train_dataset.tensors[0]\n",
    "train_dataset.targets = train_dataset.tensors[1]\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "test_dataset.data = test_dataset.tensors[0]\n",
    "test_dataset.targets = test_dataset.tensors[1]\n",
    "\n",
    "\n",
    "\n",
    "labels =['ARP poisining', 'BeEF HTTP exploits','Mass HTTP requests','Metasploit exploits','Normal','Port scanning','TCP flood','UDP data flood']\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.classes = labels\n",
    "test_dataset.classes = labels\n",
    "\n",
    "train_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "test_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "\n",
    "b_s=16\n",
    "\n",
    "trainset = Safeman_Filter(data=train_dataset.data,targets=train_dataset.targets)\n",
    "print('All down Train Data:', len(trainset))\n",
    "trainset.__Filter__(known=known)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=b_s, shuffle=True,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real train Data:', len(trainset))\n",
    "\n",
    "\n",
    "\n",
    "testsetA = Safeman_Filter(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetA.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_A = torch.utils.data.DataLoader(\n",
    "    testsetA, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetA Data:', len(testsetA))\n",
    "\n",
    "\n",
    "\n",
    "testsetD = Safeman_FilterD(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetD.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_D = torch.utils.data.DataLoader(\n",
    "    testsetD, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetD Data:', len(testsetD))\n",
    "\n",
    "\n",
    "testsetC = Safeman_FilterC(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetC Data:', len(testsetC))\n",
    "testsetC.__Filter__(trainknown=known)\n",
    "\n",
    "\n",
    "test_loader_C = torch.utils.data.DataLoader(\n",
    "    testsetC, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetC Data:', len(testsetC))\n",
    "\n",
    "\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import mnist\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class Modelnsl8(nn.Module):\n",
    "    def __init__(self):\n",
    "       \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, num_class) \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x4=x\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x3=x\n",
    "        feat = F.normalize(x3,dim=1)\n",
    "        feat1 = F.normalize(x4,dim=1)\n",
    "        x = self.fc4(x)\n",
    "        return feat, F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plUxoFLQABZH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, loss: 35.83327865600586\n",
      "epoch i= 1\n",
      "this train epoch end\n",
      "accuracy: 0.89\n",
      "idx: 0, loss: 32.23807144165039\n",
      "epoch i= 2\n",
      "this train epoch end\n",
      "accuracy: 0.95\n",
      "idx: 0, loss: 29.899015426635742\n",
      "epoch i= 3\n",
      "this train epoch end\n",
      "accuracy: 0.98\n",
      "idx: 0, loss: 31.890729904174805\n",
      "epoch i= 4\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 33.093963623046875\n",
      "epoch i= 5\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 34.35097885131836\n",
      "epoch i= 6\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 34.35877227783203\n",
      "epoch i= 7\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 33.04555892944336\n",
      "epoch i= 8\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 31.156490325927734\n",
      "epoch i= 9\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 33.040008544921875\n",
      "epoch i= 10\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 31.988943099975586\n",
      "epoch i= 11\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 29.58827781677246\n",
      "epoch i= 12\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 27.60129165649414\n",
      "epoch i= 13\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 27.983619689941406\n",
      "epoch i= 14\n",
      "this train epoch end\n",
      "accuracy: 0.99\n",
      "idx: 0, loss: 29.60059928894043\n",
      "epoch i= 15\n",
      "this train epoch end\n",
      "accuracy: 1.00\n",
      "training end\n",
      "DNN+CL detection model for ACC in K+1 attack detection: 0.860248447204969\n"
     ]
    }
   ],
   "source": [
    "train_loader2 = train_loader\n",
    "test_loader2 = test_loader_A\n",
    "model = Modelnsl8()\n",
    "sgd = SGD(model.parameters(), lr=1e-2)\n",
    "criterion = SupConLoss(temperature=0.7)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "all_epoch = 15\n",
    "\n",
    "targetctA=[]\n",
    "predctA=[]\n",
    "\n",
    "targetctD=[]\n",
    "predctD=[]\n",
    "\n",
    "targettrain=[]\n",
    "\n",
    "\n",
    "KresA=[]\n",
    "KlossA=[]\n",
    "\n",
    "featestA=[]\n",
    "\n",
    "featestD=[]\n",
    "\n",
    "featesttrain=[]\n",
    "\n",
    "KresD=[]\n",
    "KlossD=[]\n",
    "\n",
    "\n",
    "batch_size = b_s\n",
    "for current_epoch in range(all_epoch):\n",
    "    model.train()\n",
    "    for idx, (train_x, train_label) in enumerate(train_loader2):\n",
    "        sgd.zero_grad()\n",
    "        features1,predict_y = model(train_x.float().view(-1, 28*28))\n",
    "        \n",
    "        \n",
    "        if current_epoch==all_epoch-1:\n",
    "            featesttrain+=features1\n",
    "            targettrain+=train_label\n",
    "        \n",
    "        features=torch.cat([features1, features1], dim=0)\n",
    "        f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "        feat = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        \n",
    "        target2=train_label\n",
    "        loss2 = criterion(feat, target2)\n",
    "        loss1 = loss_fn(predict_y, train_label.long())\n",
    "        loss=loss1+loss2\n",
    "        if idx % 1000 == 0:\n",
    "            print('idx: {}, loss: {}'.format(idx, loss.sum().item()))\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "    print(\"epoch i=\",current_epoch+1)\n",
    "    print(\"this train epoch end\")\n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    model.eval()\n",
    "    for idx, (test_x, test_label) in enumerate(test_loader2):\n",
    "        _,predict_y = model(test_x.float().view(-1, 28*28))\n",
    "        predict_y=predict_y.detach()\n",
    "        predict_y = np.argmax(predict_y, axis=-1)\n",
    "        current_correct_num = predict_y == test_label\n",
    "        all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "        all_sample_num += current_correct_num.shape[0]\n",
    "    acc = all_correct_num / all_sample_num\n",
    "    print('accuracy: {:.2f}'.format(acc))\n",
    "print(\"training end\")\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "all_correct_num = 0\n",
    "all_sample_num = 0\n",
    "model.eval()\n",
    "for idx, (test_x, test_label) in enumerate(test_loader2):\n",
    "    feature1,predict_y = model(test_x.float().view(-1, 28*28))\n",
    "    featestA+=feature1\n",
    "    predict_y=predict_y.detach()\n",
    "    KresA+=predict_y \n",
    "    predict_y = np.argmax(predict_y, axis=-1)\n",
    "    targetctA+=test_label\n",
    "    predctA+=predict_y\n",
    "    current_correct_num = predict_y == test_label\n",
    "    all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "    all_sample_num += current_correct_num.shape[0]\n",
    "acc = all_correct_num / all_sample_num\n",
    "\n",
    "\n",
    "\n",
    "all_correct_num = 0\n",
    "all_sample_num = 0\n",
    "model.eval()\n",
    "for idx, (test_x, test_label) in enumerate(test_loader_D):\n",
    "    feature1,predict_y = model(test_x.float().view(-1, 28*28))\n",
    "    featestD+=feature1\n",
    "    predict_y=predict_y.detach()\n",
    "    KresD+=predict_y\n",
    "    predict_y = np.argmax(predict_y, axis=-1)\n",
    "    targetctD+=test_label\n",
    "    predctD+=predict_y\n",
    "    current_correct_num = predict_y == test_label\n",
    "    all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "    all_sample_num += current_correct_num.shape[0]\n",
    "acc = all_correct_num / all_sample_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tarad=targetctA+targetctD\n",
    "pread=predctA+predctD\n",
    "\n",
    "print(\"DNN+CL detection model for ACC in K+1 attack detection:\",accuracy_score(tarad,pread))\n",
    "\n",
    "torch.save(tarad,'./savedata/tarad1225.pkl')\n",
    "torch.save(pread,'./savedata/pread1225.pkl')\n",
    "\n",
    "torch.save(targettrain,'./savedata/targettrain1229.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Kres=KresA+KresD\n",
    "\n",
    "torch.save(Kres,'./savedata/Kres1225.pkl')\n",
    "\n",
    "featest=featestA+featestD\n",
    "\n",
    "\n",
    "Krestp=[]\n",
    "for i in range(len(Kres)):\n",
    "    Krestp.append(Kres[i].numpy())\n",
    "    \n",
    "np.savetxt(r\"./savedata/Kres1225.txt\",Krestp)\n",
    "\n",
    "torch.save(featestA,'./savedata/featestA1229.pkl')\n",
    "\n",
    "torch.save(featestD,'./savedata/featestD1229.pkl')\n",
    "\n",
    "torch.save(featest,'./savedata/featest.pkl')\n",
    "\n",
    "torch.save(featesttrain,'./savedata/featesttrain1229.pkl')\n",
    "\n",
    "torch.save(model, './savedata/Kmodel0201.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Capsule Network.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
