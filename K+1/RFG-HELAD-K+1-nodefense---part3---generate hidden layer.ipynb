{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07597ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 21:15:04) \n",
      "[GCC 7.3.0]\n",
      "1.8.0+cu111\n",
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f73d9fe6090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os, random, time, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import misc\n",
    "from scipy import ndimage, signal\n",
    "import scipy\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a5a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./savedata/RPGAN\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device ='cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda:0'\n",
    "\n",
    "\n",
    "batch_size = 16    \n",
    "bestEpoch = 4  \n",
    "lr = 0.0001 \n",
    "num_epochs = 5\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "save_dir=\"./savedata/RPGAN\"\n",
    "\n",
    "print(save_dir)    \n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "log_filename = os.path.join(save_dir, 'train.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a41c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data.............................\n",
      "torch.Size([10309, 3, 32, 32]) torch.Size([2578, 3, 32, 32]) torch.Size([10309]) torch.Size([2578])\n",
      "All down Train Data: 10309\n",
      "Real train Data: 8923\n",
      "All testsetA Data: 2578\n",
      "Real testsetA Data: 2226\n",
      "All testsetA Data: 2226\n",
      "Real testsetD Data: 352\n",
      "All testsetC Data: 2578\n",
      "Real testsetC Data: 2578\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('==> Preparing data.............................')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import MNIST, CIFAR10, CIFAR100, SVHN\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "import  numpy as np\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Safeman(Dataset):\n",
    "    \n",
    "    def __init__(self, data,targets):\n",
    "        super(Safeman, self).__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        return img, target\n",
    "\n",
    "class Safeman_Filter(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(known.index(targets[i]))\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "        \n",
    "class Safeman_FilterB(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        new_targets = []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                new_targets.append(0)\n",
    "            else:\n",
    "                new_targets.append(1)\n",
    "        self.targets = np.array(new_targets)\n",
    "        self.data = self.data\n",
    "\n",
    "class Safeman_FilterC(Safeman):\n",
    "    \n",
    "    def __Filter__(self, trainknown):\n",
    "        train_class_num=len(trainknown)\n",
    "        for i in range(0,len(self.targets)) :\n",
    "            if self.targets[i]>train_class_num:\n",
    "                self.targets[i] = train_class_num\n",
    "        self.data = self.data\n",
    "\n",
    "        \n",
    "class Safeman_FilterD(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        train_class_num=len(known)\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] not in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(train_class_num)\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "\n",
    "\n",
    "        \n",
    "def setup_seed(seed):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(8)\n",
    "\n",
    "\n",
    "\n",
    "known=[0, 1, 2,3,4]\n",
    "unknown=[ 5,6,7]\n",
    "\n",
    "num_class=len(known)\n",
    "\n",
    "\n",
    "\n",
    "X_train0 = np.load('./dataset/X_train_ukm.npy')\n",
    "y_train1 = np.load('./dataset/y_train_ukm.npy')\n",
    "X_final_test0 = np.load('./dataset/X_final_test_ukm.npy' )\n",
    "y__final_test1 = np.load('./dataset/y__final_test_ukm.npy')\n",
    "\n",
    "X_train1=[]\n",
    "X_final_test1=[]\n",
    "\n",
    "for i in range(len(y_train1)):\n",
    "    a = np.resize(X_train0[i], (3, 32, 32))\n",
    "    X_train1 += [a]\n",
    "    \n",
    "for j in range(len(y__final_test1)):\n",
    "    b = np.resize(X_final_test0[j], (3, 32, 32))\n",
    "    X_final_test1 += [b]\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train,y_test = torch.Tensor(X_train1), torch.Tensor(X_final_test1), torch.from_numpy(y_train1), torch.from_numpy(y__final_test1)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape,y_test.shape)\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_dataset.data = train_dataset.tensors[0]\n",
    "train_dataset.targets = train_dataset.tensors[1]\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "test_dataset.data = test_dataset.tensors[0]\n",
    "test_dataset.targets = test_dataset.tensors[1]\n",
    "\n",
    "\n",
    "\n",
    "labels =['ARP poisining', 'BeEF HTTP exploits','Mass HTTP requests','Metasploit exploits','Normal','Port scanning','TCP flood','UDP data flood']\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.classes = labels\n",
    "test_dataset.classes = labels\n",
    "\n",
    "train_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "test_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "\n",
    "b_s=16\n",
    "\n",
    "trainset = Safeman_Filter(data=train_dataset.data,targets=train_dataset.targets)\n",
    "print('All down Train Data:', len(trainset))\n",
    "trainset.__Filter__(known=known)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=b_s, shuffle=True,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real train Data:', len(trainset))\n",
    "\n",
    "\n",
    "\n",
    "testsetA = Safeman_Filter(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetA.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_A = torch.utils.data.DataLoader(\n",
    "    testsetA, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetA Data:', len(testsetA))\n",
    "\n",
    "\n",
    "testsetD = Safeman_FilterD(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetD.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_D = torch.utils.data.DataLoader(\n",
    "    testsetD, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetD Data:', len(testsetD))\n",
    "\n",
    "\n",
    "testsetC = Safeman_FilterC(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetC Data:', len(testsetC))\n",
    "testsetC.__Filter__(trainknown=known)\n",
    "\n",
    "\n",
    "test_loader_C = torch.utils.data.DataLoader(\n",
    "    testsetC, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetC Data:', len(testsetC))\n",
    "\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d388fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "class Generatorzy(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generatorzy, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(z_dim, 256*8*8)\n",
    "        self.g_deconv_1 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(256, 128, kernel_size=3,\n",
    "                                    stride= 2, padding=(3-2+1)//2,\n",
    "                                    output_padding = (3-2)%2), \n",
    "                          nn.BatchNorm2d(128),\n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.g_deconv_2 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(128, 64, kernel_size=3,\n",
    "                                    stride= 1, padding=(3-1+1)//2,\n",
    "                                    output_padding = (3-1)%2), \n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.g_deconv_3 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(64, 3, kernel_size=3,\n",
    "                                    stride= 2, padding=(3-2+1)//2,\n",
    "                                    output_padding = (3-2)%2),\n",
    "                          nn.Tanh()\n",
    "                          )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x).view(-1, 256, 8, 8)\n",
    "        x = self.g_deconv_1(x)\n",
    "        x = self.g_deconv_2(x)\n",
    "        x = self.g_deconv_3(x)\n",
    "        \n",
    "        return x     \n",
    "\n",
    "\n",
    "    \n",
    "class Discriminatorzy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminatorzy, self).__init__()\n",
    "        \n",
    "        self.d_conv_1 = nn.Sequential(\n",
    "                          nn.Conv2d(3, 32, kernel_size=3,\n",
    "                                    stride=2, padding=1), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.d_conv_2 = nn.Sequential(\n",
    "                          nn.Conv2d(32, 64, kernel_size=3,\n",
    "                                    stride=2, padding=1), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.d_conv_3 = nn.Sequential(\n",
    "                          nn.Conv2d(64, 128, kernel_size=3,\n",
    "                                    stride=2, padding=0), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.fc = nn.Linear(3*3*128, 1)\n",
    "        self.fczy = nn.Linear(3*3*128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.d_conv_1(x)\n",
    "        x = self.d_conv_2(x)\n",
    "        x = self.d_conv_3(x)\n",
    "        x = x.view(-1, 128*3*3)\n",
    "        x_zy = self.fczy(x)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x_zy,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3cf52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "bestEpoch =2 \n",
    "z_dim=100\n",
    "\n",
    "discriminator = Discriminatorzy()\n",
    "netDzy = discriminator.to(device)\n",
    "generator = Generatorzy(z_dim)\n",
    "netGzy = generator.to(device)\n",
    "\n",
    "\n",
    "path_to_G = os.path.join(save_dir, 'RPGAN-epoch-{}.GNet'.format(bestEpoch))\n",
    "path_to_D = os.path.join(save_dir, 'RPGAN-epoch-{}.DNet'.format(bestEpoch))\n",
    "netGzy.load_state_dict(torch.load(path_to_G))\n",
    "netDzy.load_state_dict(torch.load(path_to_D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8514fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featesttrain=[]\n",
    "\n",
    "for sample in train_loader:    \n",
    "    datazy, label = sample\n",
    "    datazy = datazy.to(device)\n",
    "    label = label.type(torch.long).view(-1).to(device)    \n",
    "    features1,_ = netDzy(datazy) \n",
    "    featesttrain+=features1\n",
    "\n",
    "torch.save(featesttrain,'./savedata/featesttrain0102.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ffcf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "featestA=[]\n",
    "\n",
    "dataloader_test_closeset= test_loader_A\n",
    "\n",
    "for sample in dataloader_test_closeset:    \n",
    "    datazy, label = sample\n",
    "    datazy = datazy.to(device)\n",
    "    label = label.type(torch.long).view(-1).to(device)    \n",
    "    features1,_ = netDzy(datazy)\n",
    "    featestA+=features1\n",
    "\n",
    "\n",
    "torch.save(featestA,'./savedata/featestA0102.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f44f068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save done!\n"
     ]
    }
   ],
   "source": [
    "featestD=[]\n",
    "\n",
    "dataloader_openset = test_loader_D\n",
    "\n",
    "for sample in dataloader_openset:\n",
    "    datazy, label = sample\n",
    "    datazy = datazy.to(device)    \n",
    "    features1,_ = netDzy(datazy) \n",
    "    featestD+=features1\n",
    "\n",
    "\n",
    "torch.save(featestD,'./savedata/featestD0102.pkl')\n",
    "\n",
    "\n",
    "print(\"save done!\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
