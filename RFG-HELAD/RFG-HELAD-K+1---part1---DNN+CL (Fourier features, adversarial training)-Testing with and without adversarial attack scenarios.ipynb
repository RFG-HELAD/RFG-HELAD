{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0md9re01ABZA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from SupConLosszy import SupConLoss\n",
    "import copy\n",
    "\n",
    "\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fourier transform\n",
    "\n",
    "import librosa\n",
    "\n",
    "def changesy(xtrain0):\n",
    "\n",
    "    sn=np.array(xtrain0)\n",
    "    sy=xtrain0.unsqueeze(1)\n",
    "    X = librosa.stft(sn, n_fft=52, hop_length=64)\n",
    "    a=np.abs(X)\n",
    "    b=np.angle(X)\n",
    "    c=[]\n",
    "    c.append(a)\n",
    "    c.append(b)\n",
    "    c.append(sy)\n",
    "    c=np.concatenate(c)\n",
    "    \n",
    "    return c\n",
    "    \n",
    "\n",
    "def changeX(xtrain):\n",
    "    yt=[]\n",
    "    countx=len(xtrain)\n",
    "    print(\"len:\",countx)\n",
    "    for i in range(countx):\n",
    "        yt.append(changesy(xtrain[i]))\n",
    "        qt=np.array(yt)\n",
    "        qt=torch.tensor(qt)\n",
    "        qt=torch.squeeze(qt)\n",
    "    return qt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data.............................\n",
      "(10309, 46) (2578, 46) (10309,) (2578,)\n",
      "len: 10309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wk/anaconda3/envs/cvaecaposr/lib/python3.6/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=52 is too small for input signal of length=46\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 2578\n",
      "torch.Size([10309, 100]) torch.Size([2578, 100]) (10309,) (2578,)\n",
      "torch.Size([10309, 1, 28, 28]) torch.Size([2578, 1, 28, 28]) torch.Size([10309]) torch.Size([2578])\n",
      "All down Train Data: 10309\n",
      "Real train Data: 8923\n",
      "All testsetA Data: 2578\n",
      "Real testsetA Data: 2226\n",
      "All testsetA Data: 2226\n",
      "Real testsetD Data: 352\n",
      "All testsetC Data: 2578\n",
      "Real testsetC Data: 2578\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('==> Preparing data.............................')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import MNIST, CIFAR10, CIFAR100, SVHN\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "import  numpy as np\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Safeman(Dataset):\n",
    "    \n",
    "    def __init__(self, data,targets):\n",
    "        super(Safeman, self).__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        return img, target\n",
    "\n",
    "class Safeman_Filter(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(known.index(targets[i]))\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "        \n",
    "class Safeman_FilterB(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        new_targets = []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                new_targets.append(0)\n",
    "            else:\n",
    "                new_targets.append(1)\n",
    "        self.targets = np.array(new_targets)\n",
    "        self.data = self.data\n",
    "\n",
    "class Safeman_FilterC(Safeman):\n",
    "    \n",
    "    def __Filter__(self, trainknown):\n",
    "        train_class_num=len(trainknown)\n",
    "        for i in range(0,len(self.targets)) :\n",
    "            if self.targets[i]>train_class_num:\n",
    "                self.targets[i] = train_class_num\n",
    "        self.data = self.data\n",
    "\n",
    "class Safeman_FilterD(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        train_class_num=len(known)\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] not in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(train_class_num)\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "\n",
    "\n",
    "\n",
    "known=[0, 1, 2,3,4]\n",
    "unknown=[ 5,6,7]\n",
    "\n",
    "num_class=len(known)\n",
    "\n",
    "\n",
    "\n",
    "X_train0 = np.load('./dataset/X_train_ukm.npy')\n",
    "y_train1 = np.load('./dataset/y_train_ukm.npy')\n",
    "X_final_test0 = np.load('./dataset/X_final_test_ukm.npy' )\n",
    "y__final_test1 = np.load('./dataset/y__final_test_ukm.npy')\n",
    "\n",
    "\n",
    "\n",
    "print(X_train0.shape, X_final_test0.shape, y_train1.shape,y__final_test1.shape)\n",
    "\n",
    "\n",
    "\n",
    "x_train11=changeX(torch.tensor(X_train0))\n",
    "\n",
    "x_test11=changeX(torch.tensor(X_final_test0))\n",
    "\n",
    "X_train00=x_train11\n",
    "X_final_test00=x_test11\n",
    "\n",
    "print(X_train00.shape, X_final_test00.shape, y_train1.shape,y__final_test1.shape)\n",
    "\n",
    "X_train1=[]\n",
    "X_final_test1=[]\n",
    "\n",
    "for i in range(len(y_train1)):\n",
    "    a = np.resize(X_train00[i], (1, 28, 28))\n",
    "    X_train1 += [a]\n",
    "    \n",
    "for j in range(len(y__final_test1)):\n",
    "    b = np.resize(X_final_test00[j], (1, 28, 28))\n",
    "    X_final_test1 += [b]\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train,y_test = torch.Tensor(X_train1), torch.Tensor(X_final_test1), torch.from_numpy(y_train1), torch.from_numpy(y__final_test1)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape,y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_dataset.data = train_dataset.tensors[0]\n",
    "train_dataset.targets = train_dataset.tensors[1]\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "test_dataset.data = test_dataset.tensors[0]\n",
    "test_dataset.targets = test_dataset.tensors[1]\n",
    "\n",
    "\n",
    "labels =['ARP poisining', 'BeEF HTTP exploits','Mass HTTP requests','Metasploit exploits','Normal','Port scanning','TCP flood','UDP data flood']\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.classes = labels\n",
    "test_dataset.classes = labels\n",
    "\n",
    "train_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "test_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "\n",
    "b_s=16\n",
    "\n",
    "trainset = Safeman_Filter(data=train_dataset.data,targets=train_dataset.targets)\n",
    "print('All down Train Data:', len(trainset))\n",
    "trainset.__Filter__(known=known)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=b_s, shuffle=True,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real train Data:', len(trainset))\n",
    "\n",
    "\n",
    "\n",
    "testsetA = Safeman_Filter(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetA.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_A = torch.utils.data.DataLoader(\n",
    "    testsetA, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetA Data:', len(testsetA))\n",
    "\n",
    "\n",
    "testsetD = Safeman_FilterD(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetD.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_D = torch.utils.data.DataLoader(\n",
    "    testsetD, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetD Data:', len(testsetD))\n",
    "\n",
    "\n",
    "testsetC = Safeman_FilterC(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetC Data:', len(testsetC))\n",
    "testsetC.__Filter__(trainknown=known)\n",
    "\n",
    "\n",
    "test_loader_C = torch.utils.data.DataLoader(\n",
    "    testsetC, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetC Data:', len(testsetC))\n",
    "\n",
    "\n",
    "print(\"done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import mnist\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class Modelnsl8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, num_class) \n",
    "    \n",
    "    def forward(self, x,trainT=False):        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x4=x\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x3=x\n",
    "        feat = F.normalize(x3,dim=1)\n",
    "        feat1 = F.normalize(x4,dim=1)\n",
    "        x = self.fc4(x)\n",
    "        if trainT == True:\n",
    "            return feat, F.log_softmax(x, dim = 1)\n",
    "        else:\n",
    "            return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plUxoFLQABZH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, loss: 35.8233528137207\n",
      "epoch i= 1\n",
      "this train epoch end\n",
      "accuracy: 0.95\n",
      "idx: 0, loss: 29.944602966308594\n",
      "epoch i= 2\n",
      "this train epoch end\n",
      "accuracy: 0.96\n",
      "idx: 0, loss: 29.68160057067871\n",
      "epoch i= 3\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 31.362390518188477\n",
      "epoch i= 4\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 33.42679977416992\n",
      "epoch i= 5\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 34.75148391723633\n",
      "epoch i= 6\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 34.43937301635742\n",
      "epoch i= 7\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 33.100341796875\n",
      "epoch i= 8\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 31.851158142089844\n",
      "epoch i= 9\n",
      "this train epoch end\n",
      "accuracy: 0.96\n",
      "idx: 0, loss: 33.985679626464844\n",
      "epoch i= 10\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 32.01341247558594\n",
      "epoch i= 11\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 29.59646987915039\n",
      "epoch i= 12\n",
      "this train epoch end\n",
      "accuracy: 0.98\n",
      "idx: 0, loss: 27.718324661254883\n",
      "epoch i= 13\n",
      "this train epoch end\n",
      "accuracy: 0.96\n",
      "idx: 0, loss: 30.491588592529297\n",
      "epoch i= 14\n",
      "this train epoch end\n",
      "accuracy: 0.97\n",
      "idx: 0, loss: 29.597389221191406\n",
      "epoch i= 15\n",
      "this train epoch end\n",
      "accuracy: 0.98\n",
      "training end\n",
      "test_loader_A accuracy 0.9784172661870504\n",
      "test_loader_A_D accuracy 0.84472049689441\n"
     ]
    }
   ],
   "source": [
    "train_loader2 = train_loader\n",
    "test_loader2 = test_loader_A\n",
    "model = Modelnsl8()\n",
    "sgd = SGD(model.parameters(), lr=1e-2)\n",
    "criterion = SupConLoss(temperature=0.7)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "all_epoch = 15 \n",
    "\n",
    "targetctA=[]\n",
    "targetctD=[]\n",
    "\n",
    "\n",
    "predctA=[]\n",
    "predctD=[]\n",
    "\n",
    "\n",
    "targettrain=[]\n",
    "KresA=[]\n",
    "KlossA=[]\n",
    "featestA=[]\n",
    "featestD=[]\n",
    "KresD=[]\n",
    "KlossD=[]\n",
    "\n",
    "#Step 1: the DNN+CL model is trained and tested without adversarial training and without adversarial attacks\n",
    "\n",
    "batch_size = b_s\n",
    "for current_epoch in range(all_epoch):\n",
    "    model.train()\n",
    "    for idx, (train_x, train_label) in enumerate(train_loader2):\n",
    "        sgd.zero_grad()\n",
    "        features1,predict_y = model(train_x.float().view(-1, 28*28),trainT=True)\n",
    "        \n",
    "        if current_epoch==all_epoch-1:\n",
    "            targettrain+=train_label\n",
    "        \n",
    "        features=torch.cat([features1, features1], dim=0)\n",
    "        f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "        feat = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        \n",
    "        target2=train_label\n",
    "        loss2 = criterion(feat, target2)\n",
    "        loss1 = loss_fn(predict_y, train_label.long())\n",
    "        loss=loss1+loss2\n",
    "        if idx % 1000 == 0:\n",
    "            print('idx: {}, loss: {}'.format(idx, loss.sum().item()))\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "    print(\"epoch i=\",current_epoch+1)\n",
    "    print(\"this train epoch end\")\n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    model.eval()\n",
    "    for idx, (test_x, test_label) in enumerate(test_loader2):\n",
    "        _,predict_y = model(test_x.float().view(-1, 28*28),trainT=True)\n",
    "        predict_y=predict_y.detach()\n",
    "        predict_y = np.argmax(predict_y, axis=-1)\n",
    "        current_correct_num = predict_y == test_label\n",
    "        all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "        all_sample_num += current_correct_num.shape[0]\n",
    "    acc = all_correct_num / all_sample_num\n",
    "    print('accuracy: {:.2f}'.format(acc))\n",
    "print(\"training end\")\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "all_correct_num = 0\n",
    "all_sample_num = 0\n",
    "model.eval()\n",
    "for idx, (test_x, test_label) in enumerate(test_loader2):\n",
    "    feature1,predict_y = model(test_x.float().view(-1, 28*28),trainT=True)\n",
    "    featestA+=feature1\n",
    "    predict_y=predict_y.detach()\n",
    "    KresA+=predict_y \n",
    "    predict_y = np.argmax(predict_y, axis=-1)\n",
    "    targetctA+=test_label\n",
    "    predctA+=predict_y\n",
    "    current_correct_num = predict_y == test_label\n",
    "    all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "    all_sample_num += current_correct_num.shape[0]\n",
    "acc = all_correct_num / all_sample_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_correct_num = 0\n",
    "all_sample_num = 0\n",
    "model.eval()\n",
    "for idx, (test_x, test_label) in enumerate(test_loader_D):\n",
    "    feature1,predict_y = model(test_x.float().view(-1, 28*28),trainT=True)\n",
    "    featestD+=feature1\n",
    "    predict_y=predict_y.detach()\n",
    "    KresD+=predict_y\n",
    "    predict_y = np.argmax(predict_y, axis=-1)\n",
    "    targetctD+=test_label\n",
    "    predctD+=predict_y\n",
    "    current_correct_num = predict_y == test_label\n",
    "    all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "    all_sample_num += current_correct_num.shape[0]\n",
    "acc = all_correct_num / all_sample_num\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"test_loader_A accuracy\",accuracy_score(targetctA,predctA))\n",
    "\n",
    "\n",
    "tarad=targetctA+targetctD\n",
    "pread=predctA+predctD\n",
    "\n",
    "print(\"test_loader_A_D accuracy\",accuracy_score(tarad,pread))\n",
    "\n",
    "torch.save(targetctA,'./savedata/targetctA1225.pkl')\n",
    "\n",
    "torch.save(tarad,'./savedata/tarad1225.pkl')\n",
    "torch.save(pread,'./savedata/pread1225.pkl')\n",
    "\n",
    "torch.save(targettrain,'./savedata/targettrain1229.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Kres=KresA+KresD\n",
    "\n",
    "torch.save(Kres,'./savedata/Kres1225.pkl')\n",
    "\n",
    "featest=featestA+featestD\n",
    "\n",
    "\n",
    "Krestp=[]\n",
    "for i in range(len(Kres)):\n",
    "    Krestp.append(Kres[i].numpy())\n",
    "    \n",
    "np.savetxt(r\"./savedata/Kres1225.txt\",Krestp)\n",
    "\n",
    "torch.save(featestA,'./savedata/featestA1229.pkl')\n",
    "\n",
    "torch.save(featestD,'./savedata/featestD1229.pkl')\n",
    "\n",
    "torch.save(featest,'./savedata/featest.pkl')\n",
    "\n",
    "\n",
    "torch.save(model, './savedata/DNNimproveorigin0105fanhuidanfftd.pkl')\n",
    "\n",
    "#Step 1 end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Time:1.486851453781128 clean_test_acc: 0.978  adv_test_acc: 0.914\n",
      "----epoch: 0 Time:5.0408830642700195 clean_train_acc: 0.962  adv_train_acc: 0.937\n",
      "epoch: 1 Time:1.1282050609588623 clean_test_acc: 0.970  adv_test_acc: 0.946\n",
      "----epoch: 1 Time:5.366103172302246 clean_train_acc: 0.965  adv_train_acc: 0.940\n",
      "epoch: 2 Time:1.3761327266693115 clean_test_acc: 0.974  adv_test_acc: 0.950\n"
     ]
    }
   ],
   "source": [
    "train_loader2 = train_loader\n",
    "test_loader2 = test_loader_A\n",
    "\n",
    "\n",
    "model=torch.load('./savedata/DNNimproveorigin0105fanhuidanfftd.pkl')\n",
    "\n",
    "sgd = SGD(model.parameters(), lr=1e-2)\n",
    "criterion = SupConLoss(temperature=0.7)\n",
    "\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "batch_size = b_s\n",
    "optimizer=sgd\n",
    "criterion3=loss_fn\n",
    "\n",
    "import foolbox\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "all_correct_num = 0\n",
    "all_sample_num = 0\n",
    "\n",
    "\n",
    "allepoch=3 \n",
    "\n",
    "predAattack=[]\n",
    "predAnoattack=[]\n",
    "targetAnoattack=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "testAyizhiadv=[]\n",
    "\n",
    "featestAzyori=[]\n",
    "featestAzyadv=[]\n",
    "\n",
    "predctAzyori=[]\n",
    "predctAzyadv=[]\n",
    "\n",
    "predctDzy=[]\n",
    "featestDzy=[]\n",
    "\n",
    "featesttrain=[]\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(allepoch):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    model=model.cuda()\n",
    "\n",
    "    test_total, test_correct, test_robustness = 0, 0, 0\n",
    "\n",
    "    fmodel = foolbox.models.PyTorchModel(model, bounds=(-255, 255))\n",
    "\n",
    "    attack = foolbox.attacks.LinfFastGradientAttack() #FGSM\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Step 3 and step 4 show that our model is tested twice, once without perturbation and once with perturbation,\n",
    "    # which is closer to a real scenario, i.e., not all traffic will be perturbed.\n",
    "    \n",
    "    for idx, (test_x, test_label) in enumerate(test_loader2):\n",
    "        \n",
    "        #Step 3: the DNN+CL model is trained and tested with adversarial training and without adversarial attacks(for last epoch)\n",
    "\n",
    "        test_x=test_x.cuda()\n",
    "        test_label=test_label.cuda()\n",
    "\n",
    "\n",
    "        features1,predict_y = model(test_x.float().view(-1, 28*28),trainT=True)\n",
    "        predict_y=predict_y.detach()\n",
    "        predict_y=predict_y.cpu()\n",
    "        predict_y = np.argmax(predict_y, axis=-1)\n",
    "        current_correct_num = predict_y == test_label.cpu()\n",
    "        if epoch == allepoch-1:\n",
    "            predAnoattack+=predict_y\n",
    "            targetAnoattack+=test_label.cpu()\n",
    "            featestAzyori+=features1\n",
    "            predctAzyori+=predict_y\n",
    "        test_correct += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "        test_total += current_correct_num.shape[0]\n",
    "        \n",
    "        \n",
    "        #Step 3 end\n",
    "        \n",
    "        \n",
    "        #Step 4: the DNN+CL model is trained and tested with adversarial training and with adversarial attacks(for last epoch)\n",
    "\n",
    "\n",
    "        _, adv_untargeted, _ = attack(fmodel, test_x.float().view(-1, 28*28), test_label, epsilons=12/255) # adversarial attack\n",
    "        features1,predict_y2 = model(adv_untargeted.float().view(-1, 28*28),trainT=True)\n",
    "        predict_y2=predict_y2.detach()\n",
    "        \n",
    "        predict_y2=predict_y2.cpu()\n",
    "        predict_y2 = np.argmax(predict_y2, axis=-1)\n",
    "        current_correct_num2 = predict_y2 == test_label.cpu()\n",
    "        if epoch == allepoch-1:\n",
    "            predAattack+=predict_y2\n",
    "            testAyizhiadv+=adv_untargeted\n",
    "            featestAzyadv+=features1\n",
    "            predctAzyadv+=predict_y2\n",
    "        test_robustness += np.sum(current_correct_num2.numpy(), axis=-1)\n",
    "        #Step 4 end\n",
    "\n",
    "\n",
    "    test_acc, test_adv_acc = test_correct / test_total, test_robustness / test_total\n",
    "\n",
    "    # Record the time on the testset\n",
    "    end_time = time.time()\n",
    "    testset_total_time = end_time - start_time\n",
    "\n",
    "    print(\"epoch: {} Time:{} clean_test_acc: {:.3f}  adv_test_acc: {:.3f}\".format(epoch,testset_total_time, test_acc, test_adv_acc))\n",
    "\n",
    "    \n",
    "    train_total, train_correct, train_robustness = 0, 0, 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    if epoch == allepoch-1:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    #Step 2: the DNN+CL model is trained with adversarial training\n",
    "\n",
    "    for idx, (test_x, test_label) in enumerate(train_loader):\n",
    "        \n",
    "\n",
    "        fmodel = foolbox.models.PyTorchModel(model, bounds=(-255, 255))\n",
    "\n",
    "        attack = foolbox.attacks.LinfFastGradientAttack()\n",
    "\n",
    "        test_x=test_x.cuda()\n",
    "        test_label=test_label.cuda()\n",
    "        \n",
    "        \n",
    "        features1,predict_y = model(test_x.float().view(-1, 28*28),trainT=True)\n",
    "        \n",
    "        if epoch==allepoch-2:\n",
    "            featesttrain+=features1\n",
    "        \n",
    "        features=torch.cat([features1, features1], dim=0)\n",
    "        f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "        feat = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "\n",
    "\n",
    "        predict_y=predict_y.detach()\n",
    "        predict_y_temp=predict_y\n",
    "        predict_y=predict_y.cpu()\n",
    "        predict_y = np.argmax(predict_y, axis=-1)\n",
    "        current_correct_num = predict_y == test_label.cpu()\n",
    "        train_correct += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "        train_total += current_correct_num.shape[0]\n",
    "\n",
    "\n",
    "        _, adv_untargeted, _ = attack(fmodel, test_x.float().view(-1, 28*28), test_label, epsilons=12/255) # adversarial attack\n",
    "        \n",
    "        features2,predict_y2 = model(adv_untargeted.float().view(-1, 28*28),trainT=True)\n",
    "        features11=torch.cat([features2, features2], dim=0)\n",
    "        f11, f21 = torch.split(features11, [batch_size, batch_size], dim=0)\n",
    "        feat1 = torch.cat([f11.unsqueeze(1), f21.unsqueeze(1)], dim=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        predict_y2=predict_y2.detach()\n",
    "        predict_y_temp2=predict_y2\n",
    "        predict_y2=predict_y2.cpu()\n",
    "        predict_y2 = np.argmax(predict_y2, axis=-1)\n",
    "        current_correct_num2 = predict_y2 == test_label.cpu()\n",
    "        train_robustness += np.sum(current_correct_num2.numpy(), axis=-1)\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        adv_loss = criterion3(predict_y_temp, test_label)\n",
    "        clean_loss = criterion3(predict_y_temp2, test_label)\n",
    "        loss0 = criterion(feat, test_label)\n",
    "        loss1 = criterion(feat1, test_label)\n",
    "        sgd.zero_grad()\n",
    "        loss = clean_loss + adv_loss+loss0+loss1\n",
    "        loss.requires_grad_()\n",
    "        loss.backward()\n",
    "\n",
    "        sgd.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "    train_acc, train_adv_acc = train_correct / train_total, train_robustness / train_total\n",
    "    \n",
    "    #Step 2 end\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Record the time on the testset\n",
    "    end_time = time.time()\n",
    "    trainset_total_time = end_time - start_time\n",
    "\n",
    "    print(\"----epoch: {} Time:{} clean_train_acc: {:.3f}  adv_train_acc: {:.3f}\".format(epoch, trainset_total_time, train_acc, train_adv_acc))\n",
    "    \n",
    "\n",
    "torch.save(model, './savedata/DNNimproveorigin1129fanhuidan_adv.pkl')\n",
    "\n",
    "torch.save(predAnoattack,'./savedata/predAnoattack0103.pkl')\n",
    "\n",
    "torch.save(predAattack,'./savedata/predAattack0103.pkl')\n",
    "\n",
    "torch.save(targetAnoattack,'./savedata/targetAnoattack0103-0102.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Step 5: Testing unknown attacks using DNN+CL models (after adversarial training)\n",
    "\n",
    "all_correct_num = 0\n",
    "all_sample_num = 0\n",
    "model.eval()\n",
    "for idx, (test_x, test_label) in enumerate(test_loader_D):\n",
    "    \n",
    "    test_x=test_x.cuda()\n",
    "    test_label=test_label.cuda()\n",
    "    feature1,predict_y = model(test_x.float().view(-1, 28*28),trainT=True)\n",
    "    featestDzy+=feature1\n",
    "    predict_y=predict_y.detach()\n",
    "    predict_y=predict_y.cpu()\n",
    "    predict_y = np.argmax(predict_y, axis=-1)\n",
    "    predctDzy+=predict_y\n",
    "    current_correct_num = predict_y == test_label.cpu()\n",
    "    all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "    all_sample_num += current_correct_num.shape[0]\n",
    "acc = all_correct_num / all_sample_num\n",
    "\n",
    "#Step 5 end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preadzyori=predctAzyori+predctDzy\n",
    "\n",
    "preadzyadv=predctAzyadv+predctDzy\n",
    "\n",
    "featestzyori=featestAzyori+featestDzy\n",
    "\n",
    "featestzyadv=featestAzyadv+featestDzy\n",
    "\n",
    "\n",
    "#-----adv\n",
    "\n",
    "torch.save(testAyizhiadv,'./savedata/testAyizhiadv0106.pkl')\n",
    "\n",
    "#------pred\n",
    "torch.save(predctAzyori,'./savedata/predctAzyori0106.pkl')\n",
    "\n",
    "torch.save(predctAzyadv,'./savedata/predctAzyadv0106.pkl')\n",
    "\n",
    "torch.save(predctDzy,'./savedata/predctDzy0106.pkl')\n",
    "\n",
    "torch.save(preadzyori,'./savedata/preadzyori0106.pkl')\n",
    "\n",
    "torch.save(preadzyadv,'./savedata/preadzyadv0106.pkl')\n",
    "\n",
    "#------feat\n",
    "torch.save(featestAzyori,'./savedata/featestAzyori0106.pkl')\n",
    "\n",
    "torch.save(featestAzyadv,'./savedata/featestAzyadv0106.pkl')\n",
    "\n",
    "torch.save(featestDzy,'./savedata/featestDzy0106.pkl')\n",
    "\n",
    "torch.save(featestzyori,'./savedata/featestzyori0106.pkl')\n",
    "\n",
    "torch.save(featestzyadv,'./savedata/featestzyadv0106.pkl')\n",
    "\n",
    "\n",
    "#------train\n",
    "torch.save(featesttrain,'./savedata/featesttrain1229.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Capsule Network.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
