{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07597ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 21:15:04) \n",
      "[GCC 7.3.0]\n",
      "1.8.0+cu111\n",
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4f91ca8090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os, random, time, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import misc\n",
    "from scipy import ndimage, signal\n",
    "import scipy\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a5a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./savedata/RPGAN\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device ='cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda:0'\n",
    "\n",
    "\n",
    "batch_size = 16    \n",
    "bestEpoch = 4  \n",
    "lr = 0.0001 \n",
    "num_epochs = 5\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "save_dir=\"./savedata/RPGAN\"\n",
    "\n",
    "print(save_dir)    \n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "log_filename = os.path.join(save_dir, 'train.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c64b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fourier transform\n",
    "\n",
    "import librosa\n",
    "\n",
    "def changesy(xtrain0):\n",
    "\n",
    "    sn=np.array(xtrain0)\n",
    "    sy=xtrain0.unsqueeze(1)\n",
    "    X = librosa.stft(sn, n_fft=52, hop_length=64)\n",
    "    a=np.abs(X)\n",
    "    b=np.angle(X)\n",
    "    c=[]\n",
    "    c.append(a)\n",
    "    c.append(b)\n",
    "    c.append(sy)\n",
    "    c=np.concatenate(c)\n",
    "    \n",
    "    return c\n",
    "    \n",
    "\n",
    "def changeX(xtrain):\n",
    "    yt=[]\n",
    "    countx=len(xtrain)\n",
    "    print(\"len:\",countx)\n",
    "    for i in range(countx):\n",
    "        yt.append(changesy(xtrain[i]))\n",
    "        qt=np.array(yt)\n",
    "        qt=torch.tensor(qt)\n",
    "        qt=torch.squeeze(qt)\n",
    "    return qt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8523e686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data.............................\n",
      "(10309, 46) (2578, 46) (10309,) (2578,)\n",
      "len: 10309\n",
      "len: 2578\n",
      "torch.Size([10309, 100]) torch.Size([2578, 100]) (10309,) (2578,)\n",
      "torch.Size([10309, 3, 32, 32]) torch.Size([2578, 3, 32, 32]) torch.Size([10309]) torch.Size([2578])\n",
      "All down Train Data: 10309\n",
      "Real train Data: 8923\n",
      "All testsetA Data: 2578\n",
      "Real testsetA Data: 2226\n",
      "All testsetA Data: 2226\n",
      "Real testsetD Data: 352\n",
      "All testsetC Data: 2578\n",
      "Real testsetC Data: 2578\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('==> Preparing data.............................')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import MNIST, CIFAR10, CIFAR100, SVHN\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "import  numpy as np\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Safeman(Dataset):\n",
    "    \n",
    "    def __init__(self, data,targets):\n",
    "        super(Safeman, self).__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        return img, target\n",
    "\n",
    "class Safeman_Filter(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(known.index(targets[i]))\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "        \n",
    "class Safeman_FilterB(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        new_targets = []\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] in known:\n",
    "                new_targets.append(0)\n",
    "            else:\n",
    "                new_targets.append(1)\n",
    "        self.targets = np.array(new_targets)\n",
    "        self.data = self.data\n",
    "\n",
    "class Safeman_FilterC(Safeman):\n",
    "    \n",
    "    def __Filter__(self, trainknown):\n",
    "        train_class_num=len(trainknown)\n",
    "        for i in range(0,len(self.targets)) :\n",
    "            if self.targets[i]>train_class_num:\n",
    "                self.targets[i] = train_class_num\n",
    "        self.data = self.data\n",
    "\n",
    "class Safeman_FilterD(Safeman):\n",
    "\n",
    "    def __Filter__(self, known):\n",
    "        targets = self.targets.data.numpy()\n",
    "        mask, new_targets = [], []\n",
    "        train_class_num=len(known)\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i] not in known:\n",
    "                mask.append(i)\n",
    "                new_targets.append(train_class_num)\n",
    "        self.targets = np.array(new_targets)\n",
    "        mask = torch.tensor(mask).long()\n",
    "        self.data = torch.index_select(self.data, 0, mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "known=[0, 1, 2,3,4]\n",
    "unknown=[ 5,6,7]\n",
    "\n",
    "num_class=len(known)\n",
    "\n",
    "\n",
    "\n",
    "X_train0 = np.load('./dataset/X_train_ukm.npy')\n",
    "y_train1 = np.load('./dataset/y_train_ukm.npy')\n",
    "X_final_test0 = np.load('./dataset/X_final_test_ukm.npy' )\n",
    "y__final_test1 = np.load('./dataset/y__final_test_ukm.npy')\n",
    "\n",
    "\n",
    "\n",
    "print(X_train0.shape, X_final_test0.shape, y_train1.shape,y__final_test1.shape)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "x_train11=changeX(torch.tensor(X_train0))\n",
    "\n",
    "x_test11=changeX(torch.tensor(X_final_test0))\n",
    "\n",
    "X_train00=x_train11\n",
    "X_final_test00=x_test11\n",
    "\n",
    "print(X_train00.shape, X_final_test00.shape, y_train1.shape,y__final_test1.shape)\n",
    "\n",
    "X_train1=[]\n",
    "X_final_test1=[]\n",
    "\n",
    "for i in range(len(y_train1)):\n",
    "    a = np.resize(X_train0[i], (3, 32, 32))\n",
    "    X_train1 += [a]\n",
    "    \n",
    "for j in range(len(y__final_test1)):\n",
    "    b = np.resize(X_final_test0[j], (3, 32, 32))\n",
    "    X_final_test1 += [b]\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train,y_test = torch.Tensor(X_train1), torch.Tensor(X_final_test1), torch.from_numpy(y_train1), torch.from_numpy(y__final_test1)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape,y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_dataset.data = train_dataset.tensors[0]\n",
    "train_dataset.targets = train_dataset.tensors[1]\n",
    "\n",
    "\n",
    "test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "test_dataset.data = test_dataset.tensors[0]\n",
    "test_dataset.targets = test_dataset.tensors[1]\n",
    "\n",
    "\n",
    "labels =['ARP poisining', 'BeEF HTTP exploits','Mass HTTP requests','Metasploit exploits','Normal','Port scanning','TCP flood','UDP data flood']\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.classes = labels\n",
    "test_dataset.classes = labels\n",
    "\n",
    "train_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "test_dataset.classes_to_idx = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "\n",
    "b_s=16\n",
    "\n",
    "trainset = Safeman_Filter(data=train_dataset.data,targets=train_dataset.targets)\n",
    "print('All down Train Data:', len(trainset))\n",
    "trainset.__Filter__(known=known)\n",
    "\n",
    "#0930\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=b_s, shuffle=True,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real train Data:', len(trainset))\n",
    "\n",
    "\n",
    "\n",
    "testsetA = Safeman_Filter(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetA.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_A = torch.utils.data.DataLoader(\n",
    "    testsetA, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetA Data:', len(testsetA))\n",
    "\n",
    "\n",
    "testsetD = Safeman_FilterD(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetA Data:', len(testsetA))\n",
    "testsetD.__Filter__(known=known)\n",
    "\n",
    "\n",
    "test_loader_D = torch.utils.data.DataLoader(\n",
    "    testsetD, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetD Data:', len(testsetD))\n",
    "\n",
    "\n",
    "testsetC = Safeman_FilterC(data=test_dataset.data,targets=test_dataset.targets)\n",
    "print('All testsetC Data:', len(testsetC))\n",
    "testsetC.__Filter__(trainknown=known)\n",
    "\n",
    "\n",
    "test_loader_C = torch.utils.data.DataLoader(\n",
    "    testsetC, batch_size=b_s, shuffle=False,\n",
    "    num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "print('Real testsetC Data:', len(testsetC))\n",
    "\n",
    "\n",
    "print(\"done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d388fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "class Generatorzy(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generatorzy, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(z_dim, 256*8*8)\n",
    "        self.g_deconv_1 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(256, 128, kernel_size=3,\n",
    "                                    stride= 2, padding=(3-2+1)//2,\n",
    "                                    output_padding = (3-2)%2), \n",
    "                          nn.BatchNorm2d(128),\n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.g_deconv_2 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(128, 64, kernel_size=3,\n",
    "                                    stride= 1, padding=(3-1+1)//2,\n",
    "                                    output_padding = (3-1)%2), \n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.g_deconv_3 = nn.Sequential(\n",
    "                          nn.ConvTranspose2d(64, 3, kernel_size=3,\n",
    "                                    stride= 2, padding=(3-2+1)//2,\n",
    "                                    output_padding = (3-2)%2),\n",
    "                          nn.Tanh()\n",
    "                          )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x).view(-1, 256, 8, 8)\n",
    "        x = self.g_deconv_1(x)\n",
    "        x = self.g_deconv_2(x)\n",
    "        x = self.g_deconv_3(x)\n",
    "        \n",
    "        return x     \n",
    "\n",
    "\n",
    "    \n",
    "class Discriminatorzy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminatorzy, self).__init__()\n",
    "        \n",
    "        self.d_conv_1 = nn.Sequential(\n",
    "                          nn.Conv2d(3, 32, kernel_size=3,\n",
    "                                    stride=2, padding=1), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.d_conv_2 = nn.Sequential(\n",
    "                          nn.Conv2d(32, 64, kernel_size=3,\n",
    "                                    stride=2, padding=1), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.d_conv_3 = nn.Sequential(\n",
    "                          nn.Conv2d(64, 128, kernel_size=3,\n",
    "                                    stride=2, padding=0), \n",
    "                          nn.LeakyReLU()\n",
    "                          )\n",
    "        self.fc = nn.Linear(3*3*128, 1)\n",
    "        self.fczy = nn.Linear(3*3*128, 64)\n",
    "        self.fcadv = nn.Linear(3*3*128, num_class)\n",
    "\n",
    "    def forward(self, x,trainT=False):\n",
    "        x = self.d_conv_1(x)\n",
    "        x = self.d_conv_2(x)\n",
    "        x = self.d_conv_3(x)\n",
    "        x = x.view(-1, 128*3*3)\n",
    "        x_zy = self.fczy(x)\n",
    "        x_adv=self.fcadv(x)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        if trainT == False:\n",
    "            return F.log_softmax(x_adv, dim = 1)\n",
    "        else:\n",
    "            return x_zy,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8304b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)     \n",
    "        \n",
    "\n",
    "z_dim=100\n",
    "batch_size=b_s\n",
    "discriminator = Discriminatorzy()\n",
    "netDzy = discriminator.to(device)\n",
    "\n",
    "discriminatoraux = Discriminatorzy()\n",
    "netDzyaux = discriminatoraux.to(device)\n",
    "\n",
    "generator = Generatorzy(z_dim)\n",
    "netGzy = generator.to(device)\n",
    "\n",
    "\n",
    "netDzy.apply(weights_init)\n",
    "\n",
    "netDzyaux.apply(weights_init)\n",
    "\n",
    "netGzy.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "beta1 = 0.5\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netDzy.parameters(), lr=lr/1.5, betas=(beta1, 0.999))\n",
    "\n",
    "optimizerDaux = optim.Adam(netDzyaux.parameters(), lr=lr/1.5, betas=(beta1, 0.999))\n",
    "\n",
    "optimizerG = optim.Adam(netGzy.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f51eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/557]\terrD_real: 0.6909\terrDaux_real: 0.6984\terrD_fake: 0.6962\terrDaux_fake: 0.6879 \terrGqiuhe: 0.7605\n",
      "[0/5][50/557]\terrD_real: 0.6942\terrDaux_real: 0.6847\terrD_fake: 0.6894\terrDaux_fake: 0.6917 \terrGqiuhe: 0.7669\n",
      "[0/5][100/557]\terrD_real: 0.6788\terrDaux_real: 0.6666\terrD_fake: 0.6948\terrDaux_fake: 0.6751 \terrGqiuhe: 0.7650\n",
      "[0/5][150/557]\terrD_real: 0.6754\terrDaux_real: 0.6352\terrD_fake: 0.6851\terrDaux_fake: 0.6553 \terrGqiuhe: 0.7781\n",
      "[0/5][200/557]\terrD_real: 0.6943\terrDaux_real: 0.6172\terrD_fake: 0.6796\terrDaux_fake: 0.6311 \terrGqiuhe: 0.7847\n",
      "[0/5][250/557]\terrD_real: 0.6919\terrDaux_real: 0.6920\terrD_fake: 0.6900\terrDaux_fake: 0.6974 \terrGqiuhe: 0.7675\n",
      "[0/5][300/557]\terrD_real: 0.6961\terrDaux_real: 0.7335\terrD_fake: 0.6925\terrDaux_fake: 0.6979 \terrGqiuhe: 0.7636\n",
      "[0/5][350/557]\terrD_real: 0.6917\terrDaux_real: 0.6859\terrD_fake: 0.6933\terrDaux_fake: 0.6825 \terrGqiuhe: 0.7644\n",
      "[0/5][400/557]\terrD_real: 0.6941\terrDaux_real: 0.6957\terrD_fake: 0.6901\terrDaux_fake: 0.6779 \terrGqiuhe: 0.7677\n",
      "[0/5][450/557]\terrD_real: 0.6935\terrDaux_real: 0.6851\terrD_fake: 0.6923\terrDaux_fake: 0.6787 \terrGqiuhe: 0.7655\n",
      "[0/5][500/557]\terrD_real: 0.6939\terrDaux_real: 0.6945\terrD_fake: 0.6904\terrDaux_fake: 0.6786 \terrGqiuhe: 0.7674\n",
      "[0/5][550/557]\terrD_real: 0.6902\terrDaux_real: 0.6661\terrD_fake: 0.6908\terrDaux_fake: 0.6874 \terrGqiuhe: 0.7666\n",
      "[1/5][0/557]\terrD_real: 0.6880\terrDaux_real: 0.6881\terrD_fake: 0.6904\terrDaux_fake: 0.6768 \terrGqiuhe: 0.7679\n",
      "[1/5][50/557]\terrD_real: 0.6863\terrDaux_real: 0.6911\terrD_fake: 0.6949\terrDaux_fake: 0.6547 \terrGqiuhe: 0.7665\n",
      "[1/5][100/557]\terrD_real: 0.6742\terrDaux_real: 0.6839\terrD_fake: 0.6867\terrDaux_fake: 0.6700 \terrGqiuhe: 0.7775\n",
      "[1/5][150/557]\terrD_real: 0.6808\terrDaux_real: 0.6440\terrD_fake: 0.6905\terrDaux_fake: 0.6358 \terrGqiuhe: 0.7770\n",
      "[1/5][200/557]\terrD_real: 0.6905\terrDaux_real: 0.6490\terrD_fake: 0.6837\terrDaux_fake: 0.6376 \terrGqiuhe: 0.7769\n",
      "[1/5][250/557]\terrD_real: 0.6792\terrDaux_real: 0.5538\terrD_fake: 0.6757\terrDaux_fake: 0.6321 \terrGqiuhe: 0.8044\n",
      "[1/5][300/557]\terrD_real: 0.6892\terrDaux_real: 0.5526\terrD_fake: 0.6700\terrDaux_fake: 0.5903 \terrGqiuhe: 0.8125\n",
      "[1/5][350/557]\terrD_real: 0.6687\terrDaux_real: 0.6317\terrD_fake: 0.6478\terrDaux_fake: 0.5588 \terrGqiuhe: 0.8382\n",
      "[1/5][400/557]\terrD_real: 0.6559\terrDaux_real: 0.5140\terrD_fake: 0.6737\terrDaux_fake: 0.6632 \terrGqiuhe: 0.8114\n",
      "[1/5][450/557]\terrD_real: 0.6537\terrDaux_real: 0.5290\terrD_fake: 0.6320\terrDaux_fake: 0.5044 \terrGqiuhe: 0.8659\n",
      "[1/5][500/557]\terrD_real: 0.6162\terrDaux_real: 0.6845\terrD_fake: 0.6131\terrDaux_fake: 0.4058 \terrGqiuhe: 0.8470\n",
      "[1/5][550/557]\terrD_real: 0.7559\terrDaux_real: 0.5534\terrD_fake: 0.4874\terrDaux_fake: 0.4529 \terrGqiuhe: 0.7853\n",
      "[2/5][0/557]\terrD_real: 0.5825\terrDaux_real: 0.5219\terrD_fake: 0.6449\terrDaux_fake: 0.4088 \terrGqiuhe: 0.9294\n",
      "[2/5][50/557]\terrD_real: 0.4998\terrDaux_real: 0.2347\terrD_fake: 0.6984\terrDaux_fake: 0.6514 \terrGqiuhe: 0.9907\n",
      "[2/5][100/557]\terrD_real: 0.8198\terrDaux_real: 0.5097\terrD_fake: 0.4373\terrDaux_fake: 0.2497 \terrGqiuhe: 0.7541\n",
      "[2/5][150/557]\terrD_real: 0.5992\terrDaux_real: 0.4579\terrD_fake: 0.4474\terrDaux_fake: 0.1967 \terrGqiuhe: 1.0230\n",
      "[2/5][200/557]\terrD_real: 0.5013\terrDaux_real: 0.3355\terrD_fake: 0.6266\terrDaux_fake: 0.3896 \terrGqiuhe: 0.9918\n",
      "[2/5][250/557]\terrD_real: 0.4783\terrDaux_real: 0.3942\terrD_fake: 0.5289\terrDaux_fake: 0.3204 \terrGqiuhe: 1.1663\n",
      "[2/5][300/557]\terrD_real: 0.5018\terrDaux_real: 0.2535\terrD_fake: 0.6428\terrDaux_fake: 0.4378 \terrGqiuhe: 1.0393\n",
      "[2/5][350/557]\terrD_real: 0.5833\terrDaux_real: 0.4072\terrD_fake: 0.4703\terrDaux_fake: 0.2466 \terrGqiuhe: 0.9700\n",
      "[2/5][400/557]\terrD_real: 0.5960\terrDaux_real: 0.4322\terrD_fake: 0.3709\terrDaux_fake: 0.1723 \terrGqiuhe: 1.0722\n",
      "[2/5][450/557]\terrD_real: 0.4624\terrDaux_real: 0.2240\terrD_fake: 0.5190\terrDaux_fake: 0.4219 \terrGqiuhe: 1.1459\n",
      "[2/5][500/557]\terrD_real: 0.6944\terrDaux_real: 0.4048\terrD_fake: 0.3477\terrDaux_fake: 0.2509 \terrGqiuhe: 0.8266\n",
      "[2/5][550/557]\terrD_real: 0.6159\terrDaux_real: 0.2739\terrD_fake: 0.4201\terrDaux_fake: 0.3498 \terrGqiuhe: 1.0351\n",
      "[3/5][0/557]\terrD_real: 0.6244\terrDaux_real: 0.3092\terrD_fake: 0.5050\terrDaux_fake: 0.2027 \terrGqiuhe: 0.9487\n",
      "[3/5][50/557]\terrD_real: 0.6061\terrDaux_real: 0.4001\terrD_fake: 0.3952\terrDaux_fake: 0.1831 \terrGqiuhe: 0.9152\n",
      "[3/5][100/557]\terrD_real: 0.3751\terrDaux_real: 0.1722\terrD_fake: 0.6069\terrDaux_fake: 0.4910 \terrGqiuhe: 1.2013\n",
      "[3/5][150/557]\terrD_real: 0.4681\terrDaux_real: 0.2190\terrD_fake: 0.4392\terrDaux_fake: 0.3396 \terrGqiuhe: 1.1984\n",
      "[3/5][200/557]\terrD_real: 0.6161\terrDaux_real: 0.4213\terrD_fake: 0.3185\terrDaux_fake: 0.1462 \terrGqiuhe: 1.2273\n",
      "[3/5][250/557]\terrD_real: 0.4398\terrDaux_real: 0.3195\terrD_fake: 0.5101\terrDaux_fake: 0.2301 \terrGqiuhe: 1.3307\n",
      "[3/5][300/557]\terrD_real: 0.5668\terrDaux_real: 0.2639\terrD_fake: 0.3294\terrDaux_fake: 0.1955 \terrGqiuhe: 1.1682\n",
      "[3/5][350/557]\terrD_real: 0.5410\terrDaux_real: 0.2196\terrD_fake: 0.4371\terrDaux_fake: 0.2740 \terrGqiuhe: 0.9626\n",
      "[3/5][400/557]\terrD_real: 0.3758\terrDaux_real: 0.3669\terrD_fake: 0.4793\terrDaux_fake: 0.2118 \terrGqiuhe: 1.6897\n",
      "[3/5][450/557]\terrD_real: 0.5900\terrDaux_real: 0.3879\terrD_fake: 0.2960\terrDaux_fake: 0.1535 \terrGqiuhe: 1.2980\n",
      "[3/5][500/557]\terrD_real: 0.4373\terrDaux_real: 0.3548\terrD_fake: 0.4758\terrDaux_fake: 0.3180 \terrGqiuhe: 1.1162\n",
      "[3/5][550/557]\terrD_real: 0.5164\terrDaux_real: 0.2091\terrD_fake: 0.3592\terrDaux_fake: 0.1507 \terrGqiuhe: 1.2086\n",
      "[4/5][0/557]\terrD_real: 0.5455\terrDaux_real: 0.2303\terrD_fake: 0.4295\terrDaux_fake: 0.3130 \terrGqiuhe: 1.1621\n",
      "[4/5][50/557]\terrD_real: 0.3722\terrDaux_real: 0.1307\terrD_fake: 0.5332\terrDaux_fake: 0.3479 \terrGqiuhe: 1.5447\n",
      "[4/5][100/557]\terrD_real: 0.6404\terrDaux_real: 0.4193\terrD_fake: 0.3376\terrDaux_fake: 0.1802 \terrGqiuhe: 1.1586\n",
      "[4/5][150/557]\terrD_real: 0.5084\terrDaux_real: 0.2368\terrD_fake: 0.2821\terrDaux_fake: 0.1265 \terrGqiuhe: 1.4031\n",
      "[4/5][200/557]\terrD_real: 0.4420\terrDaux_real: 0.2358\terrD_fake: 0.3878\terrDaux_fake: 0.1559 \terrGqiuhe: 1.3266\n",
      "[4/5][250/557]\terrD_real: 0.5588\terrDaux_real: 0.3104\terrD_fake: 0.3371\terrDaux_fake: 0.1639 \terrGqiuhe: 1.1210\n",
      "[4/5][300/557]\terrD_real: 0.4041\terrDaux_real: 0.0845\terrD_fake: 0.4442\terrDaux_fake: 0.4606 \terrGqiuhe: 1.3425\n",
      "[4/5][350/557]\terrD_real: 0.3060\terrDaux_real: 0.1869\terrD_fake: 0.5463\terrDaux_fake: 0.4423 \terrGqiuhe: 1.2643\n",
      "[4/5][400/557]\terrD_real: 0.4626\terrDaux_real: 0.1266\terrD_fake: 0.5263\terrDaux_fake: 0.2383 \terrGqiuhe: 1.1925\n",
      "[4/5][450/557]\terrD_real: 0.4215\terrDaux_real: 0.2061\terrD_fake: 0.3504\terrDaux_fake: 0.1941 \terrGqiuhe: 1.2385\n",
      "[4/5][500/557]\terrD_real: 0.5675\terrDaux_real: 0.2579\terrD_fake: 0.3752\terrDaux_fake: 0.1893 \terrGqiuhe: 1.0416\n",
      "[4/5][550/557]\terrD_real: 0.4219\terrDaux_real: 0.3135\terrD_fake: 0.4233\terrDaux_fake: 0.2113 \terrGqiuhe: 1.2349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataloader_train_closeset=train_loader\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    i=0\n",
    "    for sample in dataloader_train_closeset:\n",
    "        data, datalabel = sample\n",
    "        ############################\n",
    "        # (1) Update D1 D2 network\n",
    "        ###########################\n",
    "\n",
    "        netDzy.zero_grad()\n",
    "        netDzyaux.zero_grad()\n",
    "\n",
    "        real_cpu = data.to(device)\n",
    "\n",
    "        bb_size = real_cpu.size(0)\n",
    "\n",
    "        label = torch.full((bb_size,), real_label, device=device)\n",
    "        labelaux = torch.full((bb_size,), fake_label, device=device)\n",
    "\n",
    "        _,output = netDzy(real_cpu,trainT=True)\n",
    "        output=output.view(-1)\n",
    "        \n",
    "        _,outputaux = netDzyaux(real_cpu,trainT=True)\n",
    "        outputaux=outputaux.view(-1)\n",
    "\n",
    "        output=output.to(torch.float32)\n",
    "        outputaux=outputaux.to(torch.float32)\n",
    "    \n",
    "    \n",
    "        label=label.to(torch.float32)\n",
    "        labelaux=labelaux.to(torch.float32)\n",
    "        \n",
    "        errD_real = criterion(output, label)        \n",
    "        errDaux_real = criterion(outputaux, labelaux)\n",
    "\n",
    "        errD_real.backward()        \n",
    "        errDaux_real.backward()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        noise = torch.randn(batch_size, z_dim, device=device)\n",
    "\n",
    "        fake = netGzy(noise)\n",
    "        \n",
    "        label.fill_(fake_label)        \n",
    "        labelaux.fill_(real_label)\n",
    "\n",
    "        _,output = netDzy(fake.detach(),trainT=True)        \n",
    "        output=output.view(-1)\n",
    "        output=output.to(torch.float32)    \n",
    "        \n",
    "        _,outputaux = netDzyaux(fake.detach(),trainT=True)\n",
    "        outputaux=outputaux.view(-1)       \n",
    "        outputaux=outputaux.to(torch.float32)\n",
    "        \n",
    "        label=label.to(torch.float32)\n",
    "        labelaux=labelaux.to(torch.float32)\n",
    "        \n",
    "        errD_fake = criterion(output, label)        \n",
    "        errDaux_fake = criterion(outputaux, labelaux)\n",
    "\n",
    "        errD_fake.backward()        \n",
    "        errDaux_fake.backward()\n",
    "        \n",
    "\n",
    "\n",
    "        optimizerD.step()\n",
    "        optimizerDaux.step()\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        netGzy.zero_grad()\n",
    "        label.fill_(real_label)  \n",
    "        \n",
    "        labelaux.fill_(fake_label) \n",
    "        \n",
    "\n",
    "        _,output = netDzy(fake,trainT=True)\n",
    "        output=output.view(-1)\n",
    "\n",
    "        output=output.to(torch.float32)\n",
    "        label=label.to(torch.float32)\n",
    "        errG = criterion(output, label)\n",
    "        \n",
    "\n",
    "        _,outputaux = netDzyaux(fake,trainT=True)\n",
    "        outputaux=outputaux.view(-1)\n",
    "\n",
    "        outputaux=outputaux.to(torch.float32)\n",
    "        labelaux=labelaux.to(torch.float32)\n",
    "        errGaux = criterion(outputaux, labelaux)\n",
    "        \n",
    "\n",
    "\n",
    "        errGqiuhe=errG+errGaux*0.1\n",
    "        errGqiuhe.backward()\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\terrD_real: %.4f\\terrDaux_real: %.4f\\terrD_fake: %.4f\\terrDaux_fake: %.4f \\terrGqiuhe: %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader_train_closeset),\n",
    "                     errD_real, errDaux_real,errD_fake,errDaux_fake, errGqiuhe))\n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "    cur_model_wts = copy.deepcopy(netGzy.state_dict())\n",
    "    path_to_save_paramOnly = os.path.join(save_dir, 'RPGAN-epoch-{}.GNet'.format(epoch+1))\n",
    "    torch.save(cur_model_wts, path_to_save_paramOnly)\n",
    "    \n",
    "    cur_model_wts = copy.deepcopy(netDzy.state_dict())\n",
    "    path_to_save_paramOnly = os.path.join(save_dir, 'RPGAN-epoch-{}.DNet'.format(epoch+1))\n",
    "    torch.save(cur_model_wts, path_to_save_paramOnly)\n",
    "    \n",
    "   \n",
    "    cur_model_wts = copy.deepcopy(netDzyaux.state_dict())\n",
    "    path_to_save_paramOnly = os.path.join(save_dir, 'RPGAN-epoch-{}.DauxNet'.format(epoch+1))\n",
    "    torch.save(cur_model_wts, path_to_save_paramOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6693f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
